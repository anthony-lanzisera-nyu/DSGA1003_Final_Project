{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 1: fake review detection\n",
    "Data\n",
    "This dataset includes reviews for restaurants located in New York City. Reviews include product and user id, timestamp, ratings, and a plaintext review. Yelp has a filtering algorithm in place that identifies fake/suspicious reviews and separates them into a filtered list. This Yelp dataset contains both recommended and filtered reviews. We consider them as genuine and fake, respectively. Your goal is to predict whether a review is fake or not, i.e. a binary classification task. The positive classes (+1) are fake reviews and the negative classes are genuine reviews (0). Note that the classes are imbalanced, with around 10% fake reviews. \n",
    "\n",
    "Evaluation\n",
    "Your model should output a score for each example; higher score indicates the example is more likely to be fake.\n",
    "We will evaluate the results using auROC and AP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in Dataset from Codalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "train_df_pre = pd.read_csv(path+\"/train.csv\")\n",
    "validation_df_pre = pd.read_csv(path+\"/dev.csv\")\n",
    "#HW4 has 1, -1 as labels, so convert 0 to -1\n",
    "train_df_pre['label'] = train_df_pre['label'].replace(to_replace=0,value=-1)\n",
    "validation_df_pre['label'] = validation_df_pre['label'].replace(to_replace=0,value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(train_df_pre['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should we normalize case?  e.g., does HELLO mean Hello mean hello?\n",
    "- Should we remove stop words?\n",
    "- Should we remove punctuation, special symbols?\n",
    "- Should we lemmatise?  \"There is currently no lemmatiser with a very high accuracy rate:\n",
    "e.g., caresses -> caress ponies -> poni etc.\n",
    "- Less common are error correction, converting words to parts of speech, mapping synonyms to one.  In nltk library.\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def sample_normalizer(msg):\n",
    "    #converting messages to lowercase\n",
    "    msg = msg.lower()\n",
    "    #removing stopwords\n",
    "    msg = [word for word in msg.split() if word not in stopwords]\n",
    "    #using a stemmer\n",
    "    msg = \" \".join([stemmer.stem(word) for word in msg])\n",
    "    return msg\n",
    "\n",
    "data['text'] = data['text'].apply(review_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training, remove special symbols, and remake list(review), label combos.\n",
    "train_df = []\n",
    "symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "for review, label in dict(zip(train_df_pre['review'],train_df_pre['label'])).items():\n",
    "    rvw = review.split(' ')\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), rvw)\n",
    "    words = filter(None, words)\n",
    "    r = list(words)\n",
    "    r.append(str(label))\n",
    "    train_df.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commented out since the below takes a while (2 minutes)\n",
    "#train_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = []\n",
    "symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "for review, label in dict(zip(validation_df_pre['review'],validation_df_pre['label'])).items():\n",
    "    rvw = review.split(' ')\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), rvw)\n",
    "    words = filter(None, words)\n",
    "    r = list(words)\n",
    "    r.append(str(label))\n",
    "    validation_df.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commented out since the below takes a while (2 minutes)\n",
    "#validation_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#Takes in a dataset. \n",
    "def BOW(dataset):\n",
    "    BOW_representation = {}\n",
    "    for i, review in enumerate(dataset):\n",
    "        count_this = review[:-1]\n",
    "        BOW_representation[i] = Counter(count_this)# For review i, count each word\n",
    "    return BOW_representation\n",
    "\n",
    "#One can slice and take one or many examples of a dataset, as the commented \n",
    "#out print() shows immediately below.\n",
    "#print(BOW(train_df[2:3])) #convert example 2 to BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BOW(train_df[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BOW = BOW(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_BOW[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_BOW_full = []\n",
    "\n",
    "for k, v in BOW(train_df).items():\n",
    "    temp = []\n",
    "    for a,b in v.items():\n",
    "        temp.append(b)\n",
    "    X_BOW_full.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_BOW_full[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_BOW_full[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(review) for review in X_BOW_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BOW_post = [np.pad(row, pad_width=659, mode='constant', constant_values=0) for row in X_BOW_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_BOW = np.asarray(X_BOW)\n",
    "X_BOW_matrix = np.asmatrix(np.concatenate([np.asarray(row) for row in X_BOW], axis=0))\n",
    "# array_holder = []\n",
    "# for row in X_BOW_full[0:10]:\n",
    "#     print(row)\n",
    "#     asarr = np.asarray(row)\n",
    "#     array_holder.append(asarr)\n",
    "# X_BOW_matrix = np.concatenate(([i for i in array_holder]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BOW_matrix[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_BOW_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad to 659 each row\n",
    "X_BOW_post = np.asmatrix([np.pad(row, pad_width=659, mode='constant', constant_values=0) for row in X_BOW_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BOW_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_BOW_post[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "ovr = SMOTE(random_state = 42)\n",
    "X, y \\\n",
    "= ovr.fit_resample(X_BOW_post, train_df[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotProduct(d1, d2):\n",
    "    \"\"\"\n",
    "    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n",
    "    @param dict d2: same as d1\n",
    "    @return float: the dot product between d1 and d2\n",
    "    \"\"\"\n",
    "    if len(d1) < len(d2):\n",
    "        return dotProduct(d2, d1)\n",
    "    else:\n",
    "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    \"\"\"\n",
    "    Implements d1 += scale * d2 for sparse vectors.\n",
    "    @param dict d1: the feature vector which is mutated.\n",
    "    @param float scale\n",
    "    @param dict d2: a feature vector.\n",
    "\n",
    "    NOTE: This function does not return anything, but rather\n",
    "    increments d1 in place. We do this because it is much faster to\n",
    "    change elements of d1 in place than to build a new dictionary and\n",
    "    return it.\n",
    "    \"\"\"\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + v * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos_fast(review_list, max_epoch, lam):\n",
    "    W = {}\n",
    "    epoch = 0\n",
    "    t = 1\n",
    "    s = 1\n",
    "    x = BOW(review_list)\n",
    "    y = []\n",
    "    for review in review_list:\n",
    "        y.append(int(review[-1]))\n",
    "    #Loop\n",
    "    # Use the util.increment and util.dotProduct functions in update\n",
    "    #We use the results of problem 2 here in increment()\n",
    "    while epoch < max_epoch:\n",
    "        for j in range(len(x)):\n",
    "            t += 1\n",
    "            eta_t = 1/(t*lam)\n",
    "            s -= eta_t*lam*s\n",
    "            if y[j]*dotProduct(W,x[j])*s < 1:\n",
    "                increment(W,(eta_t*y[j])/s,x[j])        \n",
    "        epoch += 1\n",
    "    W.update((x,s*y) for x,y in W.items()) #Let's update in place.\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_pegasos_fast = pegasos_fast(train_df, max_epoch = 3, lam = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_percent(review_list, weight):\n",
    "    x = BOW(review_list)\n",
    "    y = []\n",
    "    for review in review_list:\n",
    "        y.append(int(review[-1]))\n",
    "        \n",
    "    error = 0\n",
    "    for i in range(len(x)):\n",
    "        if dotProduct(weight, x[i]) < 0:\n",
    "            pred = -1\n",
    "        else:\n",
    "            pred = 1\n",
    "        if y[i] != pred:\n",
    "            error += 1\n",
    "    return error/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_list = [0.000000001, 0.00000001, 0.0000001, 0.000001,0.00001, 0.0001, 0.001, 0.01, 0.1, 1,10, 100]\n",
    "loss_list = []\n",
    "for regularizer in lam_list:\n",
    "    weight = pegasos_fast(train_df,max_epoch=10, lam = regularizer)\n",
    "    loss = accuracy_percent(validation_df,weight)\n",
    "    loss_list.append(loss)\n",
    "print('Table of each Lambda and its Loss')\n",
    "for lam, loss in zip(lam_list, loss_list):\n",
    "    print(lam, loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "def evaluate_model(review_list, weight, evaluation_metric):\n",
    "    \n",
    "    x = BOW(review_list)\n",
    "    \n",
    "    y_true_pre = []\n",
    "    for review in review_list:\n",
    "        y_true_pre.append(int(review[-1]))\n",
    "    \n",
    "    y_scores_pre = [] #the model predictions\n",
    "    for i in range(len(x)):\n",
    "        if dotProduct(weight, x[i]) < 0:\n",
    "            y_scores_pre.append(-1)\n",
    "        else:\n",
    "            y_scores_pre.append(1)\n",
    "    \n",
    "    y_true = np.array(y_true_pre)\n",
    "    y_scores = np.array(y_scores_pre)\n",
    "        \n",
    "    if evaluation_metric == 'auROC':\n",
    "        metric = roc_auc_score(y_true, y_scores)\n",
    "    elif evaluation_metric == 'AP':\n",
    "        metric = average_precision_score(y_true, y_scores)\n",
    "    return metric  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_list = [0.000000001, 0.00000001, 0.0000001, 0.000001,0.00001, 0.0001, 0.001, 0.01, 0.1, 1,10, 100]\n",
    "auROC_list = []\n",
    "AP_list = []\n",
    "for regularizer in lam_list:\n",
    "    weight = pegasos_fast(train_df,max_epoch=10, lam = regularizer)\n",
    "    auROC_metric = evaluate_model(validation_df,weight,'auROC')\n",
    "    auROC_list.append(auROC_metric)\n",
    "    AP_metric = evaluate_model(validation_df,weight,'AP')\n",
    "    AP_list.append(AP_metric)\n",
    "print('Table of each Lambda and its Evaluation Metric')\n",
    "for lam, auROC, AP in zip(lam_list, auROC_list, AP_list):\n",
    "    print(lam, auROC, AP)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
